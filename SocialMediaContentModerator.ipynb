{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faca72e-ecb1-498e-aa1f-96e4f4962f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers openai torch torchvision Pillow opencv-python google.generativeai timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3797da-5fd6-4254-94c9-574ed261ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv social\n",
    "# virtualenv social\n",
    "# source social/bin/activate\n",
    "# pip install jupyter\n",
    "# pip install ipykernel\n",
    "# python -m ipykernel install --user --name=social --display-name \"Python (social)\"\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a6d4ee-b8ce-40ab-b50b-03ae1526fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from transformers import pipeline, DetrImageProcessor, DetrForObjectDetection\n",
    "import google.generativeai as genai\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebc69d3-d672-41bb-a759-8dc154ca2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModeration:\n",
    "    def __init__(self):\n",
    "        # Text Analysis Pipelines\n",
    "        ''' \n",
    "        Text (Toxicity) Classification: Classifying posts or\n",
    "        comments as appropriate or inappropriate based on predefined\n",
    "        categories (e.g., hate speech, spam, harassment). \n",
    "        '''\n",
    "        self.toxicClassifier = pipeline(\"text-classification\", model = \"unitary/toxic-bert\", device = 0)\n",
    "        \n",
    "        '''\n",
    "        Sentiment Analysis: Analyzing the sentiment of user\n",
    "        comments to identify negative or harmful sentiments.\n",
    "        '''\n",
    "        self.sentimentClassifier = pipeline(\"sentiment-analysis\", model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\", device = 0)\n",
    "        \n",
    "        '''\n",
    "        Named Entity Recognition (NER): Identifying and categorizing\n",
    "        entities in text, such as people, organizations, or locations,\n",
    "        to flag potential privacy violations or inappropriate mentions.\n",
    "        '''\n",
    "        self.namedEntityRecognition = pipeline(\"ner\", model = \"dbmdz/bert-large-cased-finetuned-conll03-english\", device = 0)\n",
    "\n",
    "        '''\n",
    "        Text Generation: Generating responses to comments or\n",
    "        creating auto-replies for moderation actions.\n",
    "        '''\n",
    "        self.textGenerator = pipeline(\"text-generation\", model=\"gpt2\", device = 0)\n",
    "\n",
    "        '''\n",
    "        Zero-Shot Classification: Classifying text into custom categories\n",
    "        without needing to train a new model for each category.\n",
    "        '''\n",
    "        self.zeroShotClassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device = 0)\n",
    "        \n",
    "        # Image Analysis Pipelines\n",
    "        '''\n",
    "        Image Classification: Moderating images uploaded to social media\n",
    "        platforms by identifying inappropriate content (e.g., violence, nudity).\n",
    "        '''\n",
    "        self.imageClassifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\", device = 0)\n",
    "\n",
    "        '''\n",
    "        Image Classification (Not Safe for Work): Moderating images uploaded to social media\n",
    "        which are not safe for work environments, etc.\n",
    "        by identifying inappropriate content (e.g., violence, nudity).\n",
    "        '''\n",
    "        self.nsfwClassifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\", device = 0)\n",
    "        \n",
    "        '''\n",
    "        Image Segmentation: Analyzing specific parts of an image to detect unwanted\n",
    "        content or to focus on specific areas in user-uploaded images.\n",
    "        '''\n",
    "        self.segmentationPipeline = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50\", device = 0)\n",
    "\n",
    "        # Audio Analysis Pipeline\n",
    "        # Video Analysis Pipeline\n",
    "    \n",
    "    def checkToxicity(self, text):\n",
    "        result = self.toxicClassifier(text)\n",
    "        label = result[0]['label']\n",
    "        score = result[0]['score']\n",
    "        return f\"Given text is {label} with score of {round(score, 2)}\"\n",
    "\n",
    "    def analyzeSentiment(self, text):\n",
    "        result = self.sentimentClassifier(text)\n",
    "        label = result[0]['label']\n",
    "        score = result[0]['score']\n",
    "        return f\"Given text shows {label} sentiment with score of {round(score, 2)}\"\n",
    "\n",
    "    def namedEntityRecognizer(self, text):\n",
    "        result = self.namedEntityRecognition(text)\n",
    "        formattedResult = []\n",
    "        for item in result:\n",
    "            entity = f\"Detected {item[\"word\"]} as {item[\"entity\"]} with score of {round(item[\"score\"],2)}\"\n",
    "            formattedResult.append(entity)\n",
    "        return \"\\n\".join(formattedResult)\n",
    "\n",
    "    def textGeneration(self, text):\n",
    "        result = self.textGenerator(text, max_length = 100)\n",
    "        return result[0][\"generated_text\"]\n",
    "\n",
    "    def zeroShotClassification(self, text):\n",
    "        labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "        result = self.zeroShotClassifier(text, candidate_labels = labels)\n",
    "        scores = result[\"scores\"]\n",
    "        return f\"Positive Score: {round(scores[0],2)}, Negative Score: {round(scores[1], 2)} and Neutral Score: {round(scores[2],2)}\"\n",
    "            \n",
    "    def imageClassification(self, imagePath):\n",
    "        result = self.imageClassifier(imagePath)\n",
    "        formattedResult = []\n",
    "        for item in result:\n",
    "            object = f\"Detected {item[\"label\"]} with score of {round(item[\"score\"], 2)}\"\n",
    "            formattedResult.append(object)\n",
    "        return \"\\n\".join(formattedResult)\n",
    "\n",
    "    def detectNSFWContent(self, imagePath):\n",
    "        image = Image.open(imagePath)\n",
    "        result = self.nsfwClassifier(image)\n",
    "        return any(label['label'] == 'NSFW' for label in result)\n",
    "\n",
    "    def imageSegmentation(self, imagePath):\n",
    "        image = Image.open(imagePath)\n",
    "\n",
    "        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "\n",
    "        \n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.8)[0]\n",
    "\n",
    "        formattedResult = []\n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            formattedString = (\n",
    "                f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "                f\"{round(score.item(), 3)} at location {box}\"\n",
    "            )\n",
    "            formattedResult.append(formattedString)\n",
    "        return \"\\n\".join(formattedResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b893c4-62aa-4d9f-9553-1308c8fe7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIModeration:\n",
    "    def __init__(self, openAiApiKey):\n",
    "        self.client = OpenAI(api_key = openAiApiKey)\n",
    "\n",
    "    def generateSummary(self, text):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = \"gpt-4o-mini\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a amazing text summarization assistant.\"}, \n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following content:\\n\\n{text}\"}\n",
    "            ],\n",
    "            max_tokens = 50\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def detectLanguage(self, text):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = \"gpt-4o-mini\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a highly accurate language detecting assistant.\"}, \n",
    "                {\"role\": \"user\", \"content\": f\"Detect the language of the following content:\\n\\n{text}\"}\n",
    "            ],\n",
    "            max_tokens = 10\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def classifyText(self, text):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = \"gpt-4o-mini\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful content classifier assistant.\"}, \n",
    "                {\"role\": \"user\", \"content\": f\"Classify the following content into categories like Politics, Entertainment, Science, etc. :\\n\\n{text}\"}\n",
    "            ],\n",
    "            max_tokens = 30\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def decisionForToxicity(self, text):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = \"gpt-4o\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an intelligent decision maker and you need to decide if the text is toxic or not.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Make a decision whether the following text is safe for social media and give reasons for the same: \\n\\n{text}\"}\n",
    "            ],\n",
    "            max_tokens = 100\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf35ee1b-7744-44c5-a876-ca63a6b578c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiModeration:\n",
    "    def __init__(self, geminiApiKey):\n",
    "        genai.configure(api_key = geminiApiKey)\n",
    "        self.generativeModel = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    def detectObjectInImage(self, imagePath):\n",
    "        image = Image.open(imagePath)\n",
    "        prompt = \"Detect all the various objects in the given image.\"\n",
    "        response = self.generativeModel.generate_content([prompt, image])\n",
    "        return response.text\n",
    "\n",
    "    def checkImageQuality(self, imagePath):\n",
    "        image = Image.open(imagePath)\n",
    "        prompt = \"Check the image quality.\"\n",
    "        response = self.generativeModel.generate_content([prompt, image])\n",
    "        return response.text\n",
    "        \n",
    "    def checkProfanityInImage(self, imagePath):\n",
    "        image = Image.open(imagePath)\n",
    "        prompt = \"Check if there's any profanity in the image.\"\n",
    "        response = self.generativeModel.generate_content([prompt, image])\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed699f2-5bc6-49ce-ac06-918fc02a8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentModerator:\n",
    "    def __init__(self, openAiApiKey, geminiApiKey):\n",
    "        self.huggingFaceModeration = HuggingFaceModeration()\n",
    "        self.openAiModeration = OpenAIModeration(openAiApiKey)\n",
    "        self.geminiModeration = GeminiModeration(geminiApiKey)\n",
    "\n",
    "    def moderateTextContent(self, text):\n",
    "        report = {}\n",
    "        \n",
    "        # Hugging Face Moderation Tasks\n",
    "        report['Toxicity'] = self.huggingFaceModeration.checkToxicity(text)\n",
    "        report['Sentiment'] = self.huggingFaceModeration.analyzeSentiment(text)\n",
    "        report['Named Entity Recognition'] = self.huggingFaceModeration.namedEntityRecognizer(text)\n",
    "        report['Text Generation'] = self.huggingFaceModeration.textGeneration(text)\n",
    "        report['Zero Shot Classification'] = self.huggingFaceModeration.zeroShotClassification(text)\n",
    "\n",
    "        # OpenAI Moderation Tasks\n",
    "        report['Summary'] = self.openAiModeration.generateSummary(text)\n",
    "        report['Language'] = self.openAiModeration.detectLanguage(text)\n",
    "        report['Category'] = self.openAiModeration.classifyText(text)\n",
    "        report['Decision'] = self.openAiModeration.decisionForToxicity(text)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def moderateImageContent(self, imagePath):\n",
    "        report = {}\n",
    "\n",
    "        # Hugging Face Image Moderation Tasks\n",
    "        report['Image Classification'] = self.huggingFaceModeration.imageClassification(imagePath)\n",
    "        report['Contains NSFW'] = self.huggingFaceModeration.detectNSFWContent(imagePath)\n",
    "        report['Image Segmentation'] = self.huggingFaceModeration.imageSegmentation(imagePath)\n",
    "\n",
    "        # Gemini Image Moderation Tasks\n",
    "        report['Detected Objects'] = self.geminiModeration.detectObjectInImage(imagePath)\n",
    "        report['Image Quality'] = self.geminiModeration.checkImageQuality(imagePath)\n",
    "        report['Image Profanity'] = self.geminiModeration.checkProfanityInImage(imagePath)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def moderateContent(self, text = None, imagePath = None):\n",
    "        fullReport = {}\n",
    "        # If text is provided, perform text moderation\n",
    "        if text:\n",
    "            fullReport['textModeration'] = self.moderateTextContent(text)\n",
    "        # If image is provided, perform image moderation\n",
    "        if imagePath:\n",
    "            fullReport['imageModeration'] = self.moderateImageContent(imagePath)\n",
    "\n",
    "        return fullReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19aed145-4b65-46c9-9c9b-15fe2e165e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupContentModerator():\n",
    "    openAiApiKey = \"\"\n",
    "    geminiApiKey = \"\"\n",
    "    \n",
    "    moderator = ContentModerator(openAiApiKey, geminiApiKey)\n",
    "    # moderationReport = moderator.moderateContent(text = textContent, imagePath = imageContentPath)\n",
    "    return moderator\n",
    "\n",
    "def printTextModerationReport(report):\n",
    "    for key, value in report.items():\n",
    "        print(f\"{key} => {value}\\n\\n\")\n",
    "\n",
    "def printImageModerationReport(report):\n",
    "    for key, value in report.items():\n",
    "        print(f\"{key} => {value}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b204cb8a-f435-40de-b067-87362fc236ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of DetrForSegmentation were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized: ['model.bbox_attention.k_linear.bias', 'model.bbox_attention.k_linear.weight', 'model.bbox_attention.q_linear.bias', 'model.bbox_attention.q_linear.weight', 'model.detr.bbox_predictor.layers.0.bias', 'model.detr.bbox_predictor.layers.0.weight', 'model.detr.bbox_predictor.layers.1.bias', 'model.detr.bbox_predictor.layers.1.weight', 'model.detr.bbox_predictor.layers.2.bias', 'model.detr.bbox_predictor.layers.2.weight', 'model.detr.class_labels_classifier.bias', 'model.detr.class_labels_classifier.weight', 'model.detr.model.backbone.conv_encoder.model.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.0.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.downsample.0.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.1.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.1.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.1.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.1.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer1.2.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.2.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.2.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer1.2.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.0.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.downsample.0.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.1.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.1.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.1.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.1.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.2.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.2.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.2.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.2.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer2.3.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.3.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.3.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer2.3.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.0.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.downsample.0.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.1.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.1.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.1.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.1.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.2.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.2.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.2.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.2.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.3.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.3.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.3.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.3.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.4.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.4.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.4.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.4.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer3.5.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.5.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.5.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer3.5.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.0.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.downsample.0.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.1.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.1.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.1.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.1.conv3.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn1.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn1.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn1.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn2.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn2.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn2.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn2.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn3.bias', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn3.running_mean', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn3.running_var', 'model.detr.model.backbone.conv_encoder.model.layer4.2.bn3.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.2.conv1.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.2.conv2.weight', 'model.detr.model.backbone.conv_encoder.model.layer4.2.conv3.weight', 'model.detr.model.decoder.layernorm.bias', 'model.detr.model.decoder.layernorm.weight', 'model.detr.model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.detr.model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.detr.model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.detr.model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.detr.model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.detr.model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.detr.model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.detr.model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.detr.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.detr.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.detr.model.decoder.layers.0.fc1.bias', 'model.detr.model.decoder.layers.0.fc1.weight', 'model.detr.model.decoder.layers.0.fc2.bias', 'model.detr.model.decoder.layers.0.fc2.weight', 'model.detr.model.decoder.layers.0.final_layer_norm.bias', 'model.detr.model.decoder.layers.0.final_layer_norm.weight', 'model.detr.model.decoder.layers.0.self_attn.k_proj.bias', 'model.detr.model.decoder.layers.0.self_attn.k_proj.weight', 'model.detr.model.decoder.layers.0.self_attn.out_proj.bias', 'model.detr.model.decoder.layers.0.self_attn.out_proj.weight', 'model.detr.model.decoder.layers.0.self_attn.q_proj.bias', 'model.detr.model.decoder.layers.0.self_attn.q_proj.weight', 'model.detr.model.decoder.layers.0.self_attn.v_proj.bias', 'model.detr.model.decoder.layers.0.self_attn.v_proj.weight', 'model.detr.model.decoder.layers.0.self_attn_layer_norm.bias', 'model.detr.model.decoder.layers.0.self_attn_layer_norm.weight', 'model.detr.model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.detr.model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.detr.model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.detr.model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.detr.model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.detr.model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.detr.model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.detr.model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.detr.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.detr.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.detr.model.decoder.layers.1.fc1.bias', 'model.detr.model.decoder.layers.1.fc1.weight', 'model.detr.model.decoder.layers.1.fc2.bias', 'model.detr.model.decoder.layers.1.fc2.weight', 'model.detr.model.decoder.layers.1.final_layer_norm.bias', 'model.detr.model.decoder.layers.1.final_layer_norm.weight', 'model.detr.model.decoder.layers.1.self_attn.k_proj.bias', 'model.detr.model.decoder.layers.1.self_attn.k_proj.weight', 'model.detr.model.decoder.layers.1.self_attn.out_proj.bias', 'model.detr.model.decoder.layers.1.self_attn.out_proj.weight', 'model.detr.model.decoder.layers.1.self_attn.q_proj.bias', 'model.detr.model.decoder.layers.1.self_attn.q_proj.weight', 'model.detr.model.decoder.layers.1.self_attn.v_proj.bias', 'model.detr.model.decoder.layers.1.self_attn.v_proj.weight', 'model.detr.model.decoder.layers.1.self_attn_layer_norm.bias', 'model.detr.model.decoder.layers.1.self_attn_layer_norm.weight', 'model.detr.model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.detr.model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.detr.model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.detr.model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.detr.model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.detr.model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.detr.model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.detr.model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.detr.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.detr.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.detr.model.decoder.layers.2.fc1.bias', 'model.detr.model.decoder.layers.2.fc1.weight', 'model.detr.model.decoder.layers.2.fc2.bias', 'model.detr.model.decoder.layers.2.fc2.weight', 'model.detr.model.decoder.layers.2.final_layer_norm.bias', 'model.detr.model.decoder.layers.2.final_layer_norm.weight', 'model.detr.model.decoder.layers.2.self_attn.k_proj.bias', 'model.detr.model.decoder.layers.2.self_attn.k_proj.weight', 'model.detr.model.decoder.layers.2.self_attn.out_proj.bias', 'model.detr.model.decoder.layers.2.self_attn.out_proj.weight', 'model.detr.model.decoder.layers.2.self_attn.q_proj.bias', 'model.detr.model.decoder.layers.2.self_attn.q_proj.weight', 'model.detr.model.decoder.layers.2.self_attn.v_proj.bias', 'model.detr.model.decoder.layers.2.self_attn.v_proj.weight', 'model.detr.model.decoder.layers.2.self_attn_layer_norm.bias', 'model.detr.model.decoder.layers.2.self_attn_layer_norm.weight', 'model.detr.model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.detr.model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.detr.model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.detr.model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.detr.model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.detr.model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.detr.model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.detr.model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.detr.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.detr.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.detr.model.decoder.layers.3.fc1.bias', 'model.detr.model.decoder.layers.3.fc1.weight', 'model.detr.model.decoder.layers.3.fc2.bias', 'model.detr.model.decoder.layers.3.fc2.weight', 'model.detr.model.decoder.layers.3.final_layer_norm.bias', 'model.detr.model.decoder.layers.3.final_layer_norm.weight', 'model.detr.model.decoder.layers.3.self_attn.k_proj.bias', 'model.detr.model.decoder.layers.3.self_attn.k_proj.weight', 'model.detr.model.decoder.layers.3.self_attn.out_proj.bias', 'model.detr.model.decoder.layers.3.self_attn.out_proj.weight', 'model.detr.model.decoder.layers.3.self_attn.q_proj.bias', 'model.detr.model.decoder.layers.3.self_attn.q_proj.weight', 'model.detr.model.decoder.layers.3.self_attn.v_proj.bias', 'model.detr.model.decoder.layers.3.self_attn.v_proj.weight', 'model.detr.model.decoder.layers.3.self_attn_layer_norm.bias', 'model.detr.model.decoder.layers.3.self_attn_layer_norm.weight', 'model.detr.model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.detr.model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.detr.model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.detr.model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.detr.model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.detr.model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.detr.model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.detr.model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.detr.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.detr.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.detr.model.decoder.layers.4.fc1.bias', 'model.detr.model.decoder.layers.4.fc1.weight', 'model.detr.model.decoder.layers.4.fc2.bias', 'model.detr.model.decoder.layers.4.fc2.weight', 'model.detr.model.decoder.layers.4.final_layer_norm.bias', 'model.detr.model.decoder.layers.4.final_layer_norm.weight', 'model.detr.model.decoder.layers.4.self_attn.k_proj.bias', 'model.detr.model.decoder.layers.4.self_attn.k_proj.weight', 'model.detr.model.decoder.layers.4.self_attn.out_proj.bias', 'model.detr.model.decoder.layers.4.self_attn.out_proj.weight', 'model.detr.model.decoder.layers.4.self_attn.q_proj.bias', 'model.detr.model.decoder.layers.4.self_attn.q_proj.weight', 'model.detr.model.decoder.layers.4.self_attn.v_proj.bias', 'model.detr.model.decoder.layers.4.self_attn.v_proj.weight', 'model.detr.model.decoder.layers.4.self_attn_layer_norm.bias', 'model.detr.model.decoder.layers.4.self_attn_layer_norm.weight', 'model.detr.model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.detr.model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.detr.model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.detr.model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.detr.model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.detr.model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.detr.model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.detr.model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.detr.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.detr.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.detr.model.decoder.layers.5.fc1.bias', 'model.detr.model.decoder.layers.5.fc1.weight', 'model.detr.model.decoder.layers.5.fc2.bias', 'model.detr.model.decoder.layers.5.fc2.weight', 'model.detr.model.decoder.layers.5.final_layer_norm.bias', 'model.detr.model.decoder.layers.5.final_layer_norm.weight', 'model.detr.model.decoder.layers.5.self_attn.k_proj.bias', 'model.detr.model.decoder.layers.5.self_attn.k_proj.weight', 'model.detr.model.decoder.layers.5.self_attn.out_proj.bias', 'model.detr.model.decoder.layers.5.self_attn.out_proj.weight', 'model.detr.model.decoder.layers.5.self_attn.q_proj.bias', 'model.detr.model.decoder.layers.5.self_attn.q_proj.weight', 'model.detr.model.decoder.layers.5.self_attn.v_proj.bias', 'model.detr.model.decoder.layers.5.self_attn.v_proj.weight', 'model.detr.model.decoder.layers.5.self_attn_layer_norm.bias', 'model.detr.model.decoder.layers.5.self_attn_layer_norm.weight', 'model.detr.model.encoder.layers.0.fc1.bias', 'model.detr.model.encoder.layers.0.fc1.weight', 'model.detr.model.encoder.layers.0.fc2.bias', 'model.detr.model.encoder.layers.0.fc2.weight', 'model.detr.model.encoder.layers.0.final_layer_norm.bias', 'model.detr.model.encoder.layers.0.final_layer_norm.weight', 'model.detr.model.encoder.layers.0.self_attn.k_proj.bias', 'model.detr.model.encoder.layers.0.self_attn.k_proj.weight', 'model.detr.model.encoder.layers.0.self_attn.out_proj.bias', 'model.detr.model.encoder.layers.0.self_attn.out_proj.weight', 'model.detr.model.encoder.layers.0.self_attn.q_proj.bias', 'model.detr.model.encoder.layers.0.self_attn.q_proj.weight', 'model.detr.model.encoder.layers.0.self_attn.v_proj.bias', 'model.detr.model.encoder.layers.0.self_attn.v_proj.weight', 'model.detr.model.encoder.layers.0.self_attn_layer_norm.bias', 'model.detr.model.encoder.layers.0.self_attn_layer_norm.weight', 'model.detr.model.encoder.layers.1.fc1.bias', 'model.detr.model.encoder.layers.1.fc1.weight', 'model.detr.model.encoder.layers.1.fc2.bias', 'model.detr.model.encoder.layers.1.fc2.weight', 'model.detr.model.encoder.layers.1.final_layer_norm.bias', 'model.detr.model.encoder.layers.1.final_layer_norm.weight', 'model.detr.model.encoder.layers.1.self_attn.k_proj.bias', 'model.detr.model.encoder.layers.1.self_attn.k_proj.weight', 'model.detr.model.encoder.layers.1.self_attn.out_proj.bias', 'model.detr.model.encoder.layers.1.self_attn.out_proj.weight', 'model.detr.model.encoder.layers.1.self_attn.q_proj.bias', 'model.detr.model.encoder.layers.1.self_attn.q_proj.weight', 'model.detr.model.encoder.layers.1.self_attn.v_proj.bias', 'model.detr.model.encoder.layers.1.self_attn.v_proj.weight', 'model.detr.model.encoder.layers.1.self_attn_layer_norm.bias', 'model.detr.model.encoder.layers.1.self_attn_layer_norm.weight', 'model.detr.model.encoder.layers.2.fc1.bias', 'model.detr.model.encoder.layers.2.fc1.weight', 'model.detr.model.encoder.layers.2.fc2.bias', 'model.detr.model.encoder.layers.2.fc2.weight', 'model.detr.model.encoder.layers.2.final_layer_norm.bias', 'model.detr.model.encoder.layers.2.final_layer_norm.weight', 'model.detr.model.encoder.layers.2.self_attn.k_proj.bias', 'model.detr.model.encoder.layers.2.self_attn.k_proj.weight', 'model.detr.model.encoder.layers.2.self_attn.out_proj.bias', 'model.detr.model.encoder.layers.2.self_attn.out_proj.weight', 'model.detr.model.encoder.layers.2.self_attn.q_proj.bias', 'model.detr.model.encoder.layers.2.self_attn.q_proj.weight', 'model.detr.model.encoder.layers.2.self_attn.v_proj.bias', 'model.detr.model.encoder.layers.2.self_attn.v_proj.weight', 'model.detr.model.encoder.layers.2.self_attn_layer_norm.bias', 'model.detr.model.encoder.layers.2.self_attn_layer_norm.weight', 'model.detr.model.encoder.layers.3.fc1.bias', 'model.detr.model.encoder.layers.3.fc1.weight', 'model.detr.model.encoder.layers.3.fc2.bias', 'model.detr.model.encoder.layers.3.fc2.weight', 'model.detr.model.encoder.layers.3.final_layer_norm.bias', 'model.detr.model.encoder.layers.3.final_layer_norm.weight', 'model.detr.model.encoder.layers.3.self_attn.k_proj.bias', 'model.detr.model.encoder.layers.3.self_attn.k_proj.weight', 'model.detr.model.encoder.layers.3.self_attn.out_proj.bias', 'model.detr.model.encoder.layers.3.self_attn.out_proj.weight', 'model.detr.model.encoder.layers.3.self_attn.q_proj.bias', 'model.detr.model.encoder.layers.3.self_attn.q_proj.weight', 'model.detr.model.encoder.layers.3.self_attn.v_proj.bias', 'model.detr.model.encoder.layers.3.self_attn.v_proj.weight', 'model.detr.model.encoder.layers.3.self_attn_layer_norm.bias', 'model.detr.model.encoder.layers.3.self_attn_layer_norm.weight', 'model.detr.model.encoder.layers.4.fc1.bias', 'model.detr.model.encoder.layers.4.fc1.weight', 'model.detr.model.encoder.layers.4.fc2.bias', 'model.detr.model.encoder.layers.4.fc2.weight', 'model.detr.model.encoder.layers.4.final_layer_norm.bias', 'model.detr.model.encoder.layers.4.final_layer_norm.weight', 'model.detr.model.encoder.layers.4.self_attn.k_proj.bias', 'model.detr.model.encoder.layers.4.self_attn.k_proj.weight', 'model.detr.model.encoder.layers.4.self_attn.out_proj.bias', 'model.detr.model.encoder.layers.4.self_attn.out_proj.weight', 'model.detr.model.encoder.layers.4.self_attn.q_proj.bias', 'model.detr.model.encoder.layers.4.self_attn.q_proj.weight', 'model.detr.model.encoder.layers.4.self_attn.v_proj.bias', 'model.detr.model.encoder.layers.4.self_attn.v_proj.weight', 'model.detr.model.encoder.layers.4.self_attn_layer_norm.bias', 'model.detr.model.encoder.layers.4.self_attn_layer_norm.weight', 'model.detr.model.encoder.layers.5.fc1.bias', 'model.detr.model.encoder.layers.5.fc1.weight', 'model.detr.model.encoder.layers.5.fc2.bias', 'model.detr.model.encoder.layers.5.fc2.weight', 'model.detr.model.encoder.layers.5.final_layer_norm.bias', 'model.detr.model.encoder.layers.5.final_layer_norm.weight', 'model.detr.model.encoder.layers.5.self_attn.k_proj.bias', 'model.detr.model.encoder.layers.5.self_attn.k_proj.weight', 'model.detr.model.encoder.layers.5.self_attn.out_proj.bias', 'model.detr.model.encoder.layers.5.self_attn.out_proj.weight', 'model.detr.model.encoder.layers.5.self_attn.q_proj.bias', 'model.detr.model.encoder.layers.5.self_attn.q_proj.weight', 'model.detr.model.encoder.layers.5.self_attn.v_proj.bias', 'model.detr.model.encoder.layers.5.self_attn.v_proj.weight', 'model.detr.model.encoder.layers.5.self_attn_layer_norm.bias', 'model.detr.model.encoder.layers.5.self_attn_layer_norm.weight', 'model.detr.model.input_projection.bias', 'model.detr.model.input_projection.weight', 'model.detr.model.query_position_embeddings.weight', 'model.mask_head.adapter1.bias', 'model.mask_head.adapter1.weight', 'model.mask_head.adapter2.bias', 'model.mask_head.adapter2.weight', 'model.mask_head.adapter3.bias', 'model.mask_head.adapter3.weight', 'model.mask_head.gn1.bias', 'model.mask_head.gn1.weight', 'model.mask_head.gn2.bias', 'model.mask_head.gn2.weight', 'model.mask_head.gn3.bias', 'model.mask_head.gn3.weight', 'model.mask_head.gn4.bias', 'model.mask_head.gn4.weight', 'model.mask_head.gn5.bias', 'model.mask_head.gn5.weight', 'model.mask_head.lay1.bias', 'model.mask_head.lay1.weight', 'model.mask_head.lay2.bias', 'model.mask_head.lay2.weight', 'model.mask_head.lay3.bias', 'model.mask_head.lay3.weight', 'model.mask_head.lay4.bias', 'model.mask_head.lay4.weight', 'model.mask_head.lay5.bias', 'model.mask_head.lay5.weight', 'model.mask_head.out_lay.bias', 'model.mask_head.out_lay.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  1\n",
      "Enter the text:  Woke up feeling grateful for all the little things in life. 😊 Here’s to a great day ahead for everyone! Sending positive vibes to anyone who needs it today! 🌟 You’re doing amazing, keep going! I am having fun here in Paris, You should definitely see the Eiffel Tower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity => Given text is toxic with score of 0.0\n",
      "\n",
      "\n",
      "Sentiment => Given text shows positive sentiment with score of 0.99\n",
      "\n",
      "\n",
      "Named Entity Recognition => Detected Paris as I-LOC with score of 1.0\n",
      "Detected E as I-MISC with score of 0.5600000023841858\n",
      "Detected ##iff as I-LOC with score of 0.6600000262260437\n",
      "Detected ##el as I-LOC with score of 0.6499999761581421\n",
      "Detected Tower as I-LOC with score of 0.8799999952316284\n",
      "\n",
      "\n",
      "Text Generation => Woke up feeling grateful for all the little things in life. 😊 Here’s to a great day ahead for everyone! Sending positive vibes to anyone who needs it today! 🌟 You’re doing amazing, keep going! I am having fun here in Paris, You should definitely see the Eiffel Tower with everyone and thank you ❤️ ♥ Thank you for everyone's support and to all my friends from Paris! ❤️ My name is Kait\n",
      "\n",
      "\n",
      "Zero Shot Classification => Positive Score: 0.96, Negative Score: 0.03 and Neutral Score: 0.01\n",
      "\n",
      "\n",
      "Summary => The author expresses gratitude for life's small joys, sends positive vibes to others, and encourages them to keep going. They are enjoying their time in Paris and recommend visiting the Eiffel Tower.\n",
      "\n",
      "\n",
      "Language => The language of the provided content is English.\n",
      "\n",
      "\n",
      "Category => This content falls into the category of **Lifestyle**. It includes elements of gratitude, positivity, and travel experiences.\n",
      "\n",
      "\n",
      "Decision => The text is safe for social media. \n",
      "\n",
      "Reasons:\n",
      "1. Positive Language: The text expresses gratitude and positivity, which promotes a healthy and encouraging atmosphere.\n",
      "2. Encouragement: It includes supportive and uplifting messages for others, which can have a positive impact on readers.\n",
      "3. Harmless Content: The text discusses personal experiences and generic advice (visiting the Eiffel Tower), which are non-controversial and non-offensive.\n",
      "4. Absence of Harmful Elements: There is no use\n",
      "\n",
      "\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  1\n",
      "Enter the text:  \"People like you don’t belong here. Go back to where you came from.\" \"I can’t stand that group; they’re ruining everything for the rest of us.\" \"You’re a complete failure and everyone knows it. Just leave social media already.\" \"Nobody wants to hear what you have to say. You’re worthless.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity => Given text is toxic with score of 0.92\n",
      "\n",
      "\n",
      "Sentiment => Given text shows negative sentiment with score of 0.95\n",
      "\n",
      "\n",
      "Named Entity Recognition => \n",
      "\n",
      "\n",
      "Text Generation => \"People like you don’t belong here. Go back to where you came from.\" \"I can’t stand that group; they’re ruining everything for the rest of us.\" \"You’re a complete failure and everyone knows it. Just leave social media already.\" \"Nobody wants to hear what you have to say. You’re worthless.\" \"You are the worst thing for me to see being a part of the situation.\" \"We still need to work\n",
      "\n",
      "\n",
      "Zero Shot Classification => Positive Score: 0.96, Negative Score: 0.02 and Neutral Score: 0.01\n",
      "\n",
      "\n",
      "Summary => The content expresses negative and hostile sentiments directed towards individuals, suggesting feelings of exclusion, disdain, and personal attacks. It highlights themes of discrimination, social rejection, and the impact of harmful criticism, particularly in the context of social media interactions.\n",
      "\n",
      "\n",
      "Language => The content is in English.\n",
      "\n",
      "\n",
      "Category => The content provided falls under the category of **Social Issues** or **Online Behavior**. It primarily reflects themes of discrimination, negativity, and online harassment\n",
      "\n",
      "\n",
      "Decision => The text is not safe for social media. Here's why:\n",
      "\n",
      "1. **Discrimination and Xenophobia**: The statement \"People like you don’t belong here. Go back to where you came from.\" is a classic example of xenophobia and promotes discrimination against individuals based on their background or origin. Such language fosters an unwelcoming and hostile environment.\n",
      "\n",
      "2. **Hatred and Group Targeting**: The phrase \"I can’t stand that group; they’re ruining everything for the rest of\n",
      "\n",
      "\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  2\n",
      "Enter the image path:  Positive.jpeg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Classification => Detected lakeside, lakeshore with score of 0.28\n",
      "Detected pier with score of 0.15\n",
      "Detected seashore, coast, seacoast, sea-coast with score of 0.14\n",
      "Detected steel arch bridge with score of 0.13\n",
      "Detected dock, dockage, docking facility with score of 0.05\n",
      "\n",
      "\n",
      "Contains NSFW => False\n",
      "\n",
      "\n",
      "Image Segmentation => Detected boat with confidence 0.946 at location [232.54, 83.07, 256.06, 98.92]\n",
      "\n",
      "\n",
      "Detected Objects => The image shows an aerial view of a city. The following objects are present:\n",
      "\n",
      "* **Buildings:** Numerous skyscrapers and other buildings are visible, creating the urban landscape.\n",
      "* **Water:** A large body of water, likely a river or bay, surrounds the city.\n",
      "* **Land:** The city is situated on an island, which is surrounded by water.\n",
      "* **Sky:** The sky is clear and blue, suggesting a sunny day.\n",
      "* **Trees:** Some trees are visible, adding a touch of greenery to the cityscape.\n",
      "* **Roads:** Roads and highways can be seen winding through the city.\n",
      "* **Boats:** Several boats and ships are present in the water, suggesting maritime activity.\n",
      "* **Bridges:**  A few bridges can be seen connecting different parts of the city.\n",
      "* **Shoreline:** The shoreline is well-defined, with piers and docks visible.\n",
      "* **Clouds:**  Some white fluffy clouds are scattered across the sky, adding depth and dimension. \n",
      "\n",
      "\n",
      "\n",
      "Image Quality => The image quality is good. The image is clear, sharp, and well-lit. The colors are vibrant and the details are well-defined. There are no visible artifacts or compression issues. Overall, the image is of high quality. \n",
      "\n",
      "\n",
      "\n",
      "Image Profanity => I am unable to see or analyze any image. I am a text-based chat assistant and do not have the capability to process visual information. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  2\n",
      "Enter the image path:  Negative.jpeg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Classification => Detected corkscrew, bottle screw with score of 0.8\n",
      "Detected lighter, light, igniter, ignitor with score of 0.04\n",
      "Detected can opener, tin opener with score of 0.02\n",
      "Detected letter opener, paper knife, paperknife with score of 0.02\n",
      "Detected red wine with score of 0.01\n",
      "\n",
      "\n",
      "Contains NSFW => False\n",
      "\n",
      "\n",
      "Image Segmentation => Detected person with confidence 0.821 at location [0.7, 0.03, 209.76, 182.99]\n",
      "\n",
      "\n",
      "Detected Objects => The image contains a hand holding a bloody knife.  It is not appropriate to discuss or encourage violence. If you or someone you know is experiencing violence, please reach out for help. You can contact the National Domestic Violence Hotline at 1-800-799-SAFE (7233) or visit their website at https://www.thehotline.org. \n",
      "\n",
      "\n",
      "\n",
      "Image Quality => I cannot see or analyze any image. I am a text-based chat assistant and thus I cannot process any image. \n",
      "\n",
      "\n",
      "\n",
      "Image Profanity => I cannot see or analyze any image. I am a text-based chat assistant and do not have the ability to process images. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  7\n",
      "Enter the image path:  img.jpeg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Objects: The image contains the following objects:\n",
      "\n",
      "* Beach ball\n",
      "* Airplane\n",
      "* Sunglasses\n",
      "* Starfish\n",
      "* Suitcase\n",
      "* Flamingo float\n",
      "* Palm tree\n",
      "* Ice cream cone \n",
      "\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  9\n",
      "Enter the text: \"Everything you post is total garbage. Stop polluting my feed.\" \"I hate seeing people like you on my timeline. Just disappear.\" \"One day, someone’s going to make sure you pay for everything you’ve done.\" \"You’re lucky people don’t come after you. Keep talking and see what happens.\" \"That community is full of idiots. They should just stay out of our way.\" \"Why do those people always act like this? It’s so typical of them.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision for Toxicity: The text is not safe for social media. Here are the reasons:\n",
      "\n",
      "1. **Insulting Language**: The text contains derogatory terms such as \"total garbage,\" \"idiots,\" and \"people like you,\" which are disrespectful and can cause harm to individuals or groups.\n",
      "\n",
      "2. **Threatening Tone**: Phrases like \"One day, someone’s going to make sure you pay\" and \"Keep talking and see what happens\" carry an implicit threat of harm or retribution\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  5\n",
      "Enter the text: \"Everything you post is total garbage. Stop polluting my feed.\" \"I hate seeing people like you on my timeline. Just disappear.\" \"One day, someone’s going to make sure you pay for everything you’ve done.\" \"You’re lucky people don’t come after you. Keep talking and see what happens.\" \"That community is full of idiots. They should just stay out of our way.\" \"Why do those people always act like this? It’s so typical of them.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive Score: 0.93, Negative Score: 0.04 and Neutral Score: 0.03\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  4\n",
      "Enter the text: \"Everything you post is total garbage. Stop polluting my feed.\" \"I hate seeing people like you on my timeline. Just disappear.\" \"One day, someone’s going to make sure you pay for everything you’ve done.\" \"You’re lucky people don’t come after you. Keep talking and see what happens.\" \"That community is full of idiots. They should just stay out of our way.\" \"Why do those people always act like this? It’s so typical of them.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity: Given text is toxic with score of 0.96\n",
      "\n",
      "--- Social Media Content Moderator Menu ---\n",
      "1. Moderate Text Content\n",
      "2. Moderate Image Content\n",
      "3. Moderate Both Text and Image Content\n",
      "4. Find Toxicity Score of Text Content\n",
      "5. Find the Sentiment of the Text Content\n",
      "6. Identify the Entities from the Text Content\n",
      "7. Detect Objects in the Image\n",
      "8. Profanity in the Image\n",
      "9. Decision whether the Text is Safe for Social Media\n",
      "10. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select an option (1 - 10):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the content moderation tool. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    moderator = setupContentModerator()\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n--- Social Media Content Moderator Menu ---\")\n",
    "        print(\"1. Moderate Text Content\")\n",
    "        print(\"2. Moderate Image Content\")\n",
    "        print(\"3. Moderate Both Text and Image Content\")\n",
    "        print(\"4. Find Toxicity Score of Text Content\")\n",
    "        print(\"5. Find the Sentiment of the Text Content\")\n",
    "        print(\"6. Identify the Entities from the Text Content\")\n",
    "        print(\"7. Detect Objects in the Image\")\n",
    "        print(\"8. Profanity in the Image\")\n",
    "        print(\"9. Decision whether the Text is Safe for Social Media\")\n",
    "        print(\"10. Exit\")\n",
    "\n",
    "        choice = input(\"Select an option (1 - 10): \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            textContent = input(\"Enter the text: \")\n",
    "            textModerationReport = moderator.moderateTextContent(textContent)\n",
    "            printTextModerationReport(textModerationReport)\n",
    "        elif choice == '2':\n",
    "            imagePath = input(\"Enter the image path: \")\n",
    "            imageModerationReport = moderator.moderateImageContent(imagePath)\n",
    "            printImageModerationReport(imageModerationReport)\n",
    "        elif choice == '3':\n",
    "            textContent = input(\"Enter the text:\")\n",
    "            imagePath = input(\"Enter the image path: \")\n",
    "            moderationReport = moderator.moderateContent(textContent, imagePath)\n",
    "            printTextModerationReport(moderationReport['textModeration'])\n",
    "            printImageModerationReport(moderationReport['imageModeration'])\n",
    "        elif choice == '4':\n",
    "            textContent = input(\"Enter the text:\")\n",
    "            result = moderator.huggingFaceModeration.checkToxicity(textContent)\n",
    "            print(f\"Toxicity: {result}\")\n",
    "        elif choice == '5':\n",
    "            textContent = input(\"Enter the text:\")\n",
    "            result = moderator.huggingFaceModeration.zeroShotClassification(textContent)\n",
    "            print(f\"Sentiment: {result}\")\n",
    "        elif choice == '6':\n",
    "            result = moderator.huggingFaceModeration.namedEntityRecognizer(textContent)\n",
    "            print(f\"Named Entity Recognition: {result}\")\n",
    "        elif choice == '7':\n",
    "            imagePath = input(\"Enter the image path: \")\n",
    "            result = moderator.geminiModeration.detectObjectInImage(imagePath)\n",
    "            print(f\"Detected Objects: {result}\")\n",
    "        elif choice == '8':\n",
    "            imagePath = input(\"Enter the image path: \")\n",
    "            result = moderator.geminiModeration.checkProfanityInImage(imagePath)\n",
    "            print(f\"Profanity: {result}\")\n",
    "        elif choice == '9':\n",
    "            textContent = input(\"Enter the text:\")\n",
    "            result = moderator.openAiModeration.decisionForToxicity(textContent)\n",
    "            print(f\"Decision for Toxicity: {result}\")\n",
    "        elif choice == '10':\n",
    "            print(\"Exiting the content moderation tool. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Wrong choice, select a valid option from the content moderation tool!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2b498-3de1-4349-b8ab-a90f01c31de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (social)",
   "language": "python",
   "name": "social"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
